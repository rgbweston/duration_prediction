# -*- coding: utf-8 -*-
"""LR_XGB_NN_1.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1j7FanbL9NgC8MGtTLAUeaoj_r6gSPb9M

# Set paths to processed data splits
"""

train_preproc_path = pd.read_csv(f"{path}/train_preproc.csv")
val_preproc_path = pd.read_csv(f"{path}/val_preproc.csv")
test_preproc_path = pd.read_csv(f"{path}/test_preproc.csv")

"""# Google colab mount (if required)"""

# Mount Drive and set base path
drive.mount('/content/drive')
path = "/content/drive/MyDrive/UCL_Dissertation"

"""# Define metric function"""



# -*- coding: utf-8 -*-
"""LR_XGB_NN_1.1.py

# Load
"""
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from google.colab import drive

# Load preprocessed splits
df_tr = pd.read_csv(train_preproc_path)
df_va = pd.read_csv(val_preproc_path)
df_te = pd.read_csv(test_preproc_path)

# Identify target column
CANDIDATE_TARGETS = ["Actual_Length", "actual_length", "target", "Target", "y", "duration", "Duration"]
tcol = next((c for c in CANDIDATE_TARGETS if c in df_tr.columns), None)
if tcol is None:
    diff_tr = list(set(df_tr.columns) - set(df_te.columns))
    diff_va = list(set(df_va.columns) - set(df_te.columns))
    if len(diff_tr) == 1 and len(diff_va) == 1 and diff_tr[0] == diff_va[0]:
        tcol = diff_tr[0]

# Extract features and target
if tcol and tcol in df_tr.columns and tcol in df_va.columns:
    y_tr = df_tr[tcol].copy()
    y_va = df_va[tcol].copy()
    y_te = df_te[tcol].copy() if tcol in df_te.columns else None
    X_tr = df_tr.drop(columns=[tcol])
    X_va = df_va.drop(columns=[tcol])
    X_te = df_te.drop(columns=[tcol]) if tcol in df_te.columns else df_te.copy()
    target_source = f"target column detected: '{tcol}'"
else:
    y_tr = y_va = y_te = None
    X_tr, X_va, X_te = df_tr.copy(), df_va.copy(), df_te.copy()
    target_source = "no explicit target column found"

# Align feature columns
common_cols = sorted(set(X_tr.columns) & set(X_va.columns) & set(X_te.columns))
X_tr = X_tr[common_cols]
X_va = X_va[common_cols]
X_te = X_te[common_cols]

# Scale numeric non-binary columns for LR and NN
def non_binary_numeric_cols(df):
    num = df.select_dtypes(include=[np.number])
    is_binary = num.apply(lambda s: set(pd.Series(s.unique()).dropna().astype(int).unique()) <= {0,1})
    return [c for c in num.columns if not is_binary.get(c, False)]

cols_to_scale = non_binary_numeric_cols(X_tr)
scaler = StandardScaler()
X_tr_scaled = X_tr.copy()
X_va_scaled = X_va.copy()
X_te_scaled = X_te.copy()
X_tr_scaled[cols_to_scale] = scaler.fit_transform(X_tr[cols_to_scale])
X_va_scaled[cols_to_scale] = scaler.transform(X_va[cols_to_scale])
X_te_scaled[cols_to_scale] = scaler.transform(X_te[cols_to_scale])

# Print diagnostics
print("=== DATA LOADED ===")
print(target_source)
print(f"Aligned feature columns: {len(common_cols)}")
print("\n=== SHAPES ===")
print("Train X:", X_tr.shape, " | y:", None if y_tr is None else y_tr.shape)
print("Val   X:", X_va.shape, " | y:", None if y_va is None else y_va.shape)
print("Test  X:", X_te.shape, " | y:", None if y_te is None else y_te.shape)

# Install dependencies
!pip install optuna xgboost --quiet

# Set parameters
SEED = 42
N_TRIALS = 1
TIMEOUT_SECS = 3600  # 1 hour

# Universal evaluator (for test set)
import os
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from scipy.stats import pearsonr

RESULTS = []

def evaluate_model(name, model, X_test, y_test, X_scaled=None, out_dir=f"{path}/outputs"):
    os.makedirs(out_dir, exist_ok=True)
    y_true = np.asarray(y_test).ravel()
    X_test_input = X_scaled if X_scaled is not None else X_test
    y_pred = model.predict(X_test_input).ravel()

    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    def win_acc(pct):
        tol_actual = (pct/100.0) * np.abs(y_true)
        within_a = np.mean(np.abs(y_pred - y_true) <= tol_actual)
        tol_pred = (pct/100.0) * np.abs(y_pred)
        within_p = np.mean(np.abs(y_pred - y_true) <= tol_pred)
        return within_a * 100, within_p * 100

    w5a, w5p = win_acc(5)
    w10a, w10p = win_acc(10)

    metrics = {
        "Model": name, "MAE": mae, "RMSE": rmse, "R2": r2,
        "Acc±5%(actual)": w5a, "Acc±5%(pred)": w5p,
        "Acc±10%(actual)": w10a, "Acc±10%(pred)": w10p
    }
    RESULTS.append(metrics)

    # Save predictions
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    pd.DataFrame({"y_true": y_true, "y_pred": y_pred}).to_csv(
        f"{out_dir}/{name}_preds_{ts}.csv", index=False
    )

    print(f"[{name} TEST] MAE={mae:.3f} | RMSE={rmse:.3f} | R2={r2:.3f}")
    print(f"       Acc±5% (actual)={w5a:.2f}% | Acc±5% (pred)={w5p:.2f}%")
    print(f"       Acc±10% (actual)={w10a:.2f}% | Acc±10% (pred)={w10p:.2f}%")

    return metrics

# Models
import optuna
from sklearn.linear_model import Ridge, ElasticNet
from sklearn.neural_network import MLPRegressor
from xgboost import XGBRegressor
import joblib
from datetime import datetime

# 1) Ridge/ElasticNet
def objective_lr(trial):
    model_type = trial.suggest_categorical("model", ["ridge", "elasticnet"])
    if model_type == "ridge":
        model = Ridge(alpha=trial.suggest_float("alpha", 1e-6, 1e3, log=True), random_state=SEED)
    else:
        model = ElasticNet(
            alpha=trial.suggest_float("alpha", 1e-6, 1e3, log=True),
            l1_ratio=trial.suggest_float("l1_ratio", 0.0, 1.0),
            max_iter=10000, random_state=SEED
        )
    model.fit(X_tr_scaled, y_tr)
    return mean_absolute_error(y_va, model.predict(X_va_scaled))

study_lr = optuna.create_study(direction="minimize", sampler=optuna.samplers.TPESampler(seed=SEED))
study_lr.optimize(objective_lr, n_trials=N_TRIALS, timeout=TIMEOUT_SECS)

bp = study_lr.best_trial.params
best_lr = Ridge(alpha=bp["alpha"], random_state=SEED) if bp["model"] == "ridge" else \
          ElasticNet(alpha=bp["alpha"], l1_ratio=bp["l1_ratio"], max_iter=10000, random_state=SEED)
best_lr.fit(X_tr_scaled, y_tr)

# Evaluate on test set
if y_te is not None:
    evaluate_model("Linear", best_lr, X_te, y_te, X_te_scaled)
    # Save model and coefficients
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    joblib.dump(best_lr, f"{path}/outputs/linear_model_{ts}.joblib")
    coef = pd.DataFrame({"feature": X_tr.columns, "coef": best_lr.coef_}).sort_values("coef", key=np.abs, ascending=False)
    coef.to_csv(f"{path}/outputs/linear_coefs_{ts}.csv", index=False)

# 2) MLP
def objective_mlp(trial):
    layers = trial.suggest_categorical("hidden_layer_sizes", [(128,128), (256,128), (256,256,128), (512,256,128)])
    alpha = trial.suggest_float("alpha", 1e-6, 1e-2, log=True)
    lr0 = trial.suggest_float("learning_rate_init", 1e-5, 3e-2, log=True)
    bs = trial.suggest_categorical("batch_size", [64, 128, 256])

    model = MLPRegressor(
        hidden_layer_sizes=layers, activation="relu", solver="adam",
        alpha=alpha, learning_rate_init=lr0, batch_size=bs,
        max_iter=200, early_stopping=True, n_iter_no_change=15,
        random_state=SEED, verbose=False
    )
    model.fit(X_tr_scaled, y_tr)
    return mean_absolute_error(y_va, model.predict(X_va_scaled))

study_mlp = optuna.create_study(direction="minimize", sampler=optuna.samplers.TPESampler(seed=SEED))
study_mlp.optimize(objective_mlp, n_trials=N_TRIALS, timeout=TIMEOUT_SECS)

bp = study_mlp.best_trial.params
best_mlp = MLPRegressor(
    hidden_layer_sizes=bp["hidden_layer_sizes"], activation="relu", solver="adam",
    alpha=bp["alpha"], learning_rate_init=bp["learning_rate_init"],
    batch_size=bp["batch_size"], max_iter=200, early_stopping=True,
    n_iter_no_change=15, random_state=SEED, verbose=False
)
best_mlp.fit(X_tr_scaled, y_tr)

# Evaluate on test set
if y_te is not None:
    evaluate_model("MLP", best_mlp, X_te, y_te, X_te_scaled)
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    joblib.dump(best_mlp, f"{path}/outputs/mlp_model_{ts}.joblib")

# 3) XGBoost
def objective_xgb(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 50, 500),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 1e-4, 0.3, log=True),
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
        "random_state": SEED
    }
    model = XGBRegressor(**params)
    model.fit(X_tr, y_tr)
    return mean_absolute_error(y_va, model.predict(X_va))

study_xgb = optuna.create_study(direction="minimize", sampler=optuna.samplers.TPESampler(seed=SEED))
study_xgb.optimize(objective_xgb, n_trials=N_TRIALS, timeout=TIMEOUT_SECS)

bp = study_xgb.best_trial.params
best_xgb = XGBRegressor(**bp)
best_xgb.fit(X_tr, y_tr)

# Evaluate on test set
if y_te is not None:
    evaluate_model("XGBoost", best_xgb, X_te, y_te)
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    joblib.dump(best_xgb, f"{path}/outputs/xgb_model_{ts}.joblib")

# Save summary
def finalize_summary(out_dir=f"{path}/outputs"):
    if not RESULTS:
        print("[WARN] No results recorded.")
        return
    df = pd.DataFrame(RESULTS).drop_duplicates(subset=["Model"], keep="last")
    df.sort_values(["MAE", "RMSE"], ascending=[True, True], inplace=True)
    ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    df.to_csv(f"{out_dir}/model_compare_summary_{ts}.csv", index=False)
    print(f"[SAVED] Summary → {out_dir}/model_compare_summary_{ts}.csv")
    print(df)

if y_te is not None:
    finalize_summary()

