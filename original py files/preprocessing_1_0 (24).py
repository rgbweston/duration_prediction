# -*- coding: utf-8 -*-
"""Preprocessing_1.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EOIAmP0uK08ioaKPayVl-hCCh9_Ny1Yt

# Processing

## Decisions
"""

Remove_missing_50pc = True
convert_lengths_abs = True
Remove_Actual_0 = True
Remove_Prev_Op_0_and_outliers = True
Binary_Previous_Operation = True
Prev_Code_update = True
Proc_code_te_encoding = True

"""## Imports"""

import os
import re
import warnings

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.decomposition import PCA
from sklearn.model_selection import KFold, train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, silhouette_samples

warnings.filterwarnings('ignore')

"""## Load df"""

# ── Mount Google Drive ──────────────────────────────
from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import os

# ── Load raw merged dataset ─────────────────────────
path_to_merged = 'drive/MyDrive/UCL_Dissertation/merged_data.csv'
df = pd.read_csv(path_to_merged)
raw_df_shape = df.shape
raw_df_cols = df.columns.tolist()
raw_df_dtypes = df.dtypes
df_raw = df.copy()

print(f"▶ Raw df shape: {raw_df_shape}")
print(f"▶ Raw df columns ({len(raw_df_cols)}): {raw_df_cols}")

"""## Cleaning

### Dtype amend and value replacement
"""

# === DTYPE INSPECTION & COERCION ===
import pandas as pd

print("\n=== Column dtypes BEFORE ===")
before_dtypes = df.dtypes.astype(str)
print(before_dtypes.sort_index())
print("\nBEFORE summary:", before_dtypes.value_counts().to_dict())

# ----- Numeric → force numeric -----
numeric_cols = ['Year', 'Operation_Age', 'IMD_Score', 'Previous_Operation_Length']
for col in numeric_cols:
    if col in df.columns:
        df[col] = pd.to_numeric(df[col], errors="coerce")

# ----- Boolean → map to True/False with your rules -----
bool_like_cols = [
    "Sex",
    "Intended_Management",  # binary after dropping 0s
    "Heart_Condition", "Hypertension", "Obesity",
    "Diabetes", "Cancer", "Chronic_Kidney_Disease",
]
for col in bool_like_cols:
    if col not in df.columns:
        continue
    s = df[col]
    if s.dtype.kind in "biufc":  # numeric-ish
        df[col] = s.map({1: True, 2: False, 0: False}).astype("boolean")
    else:  # string-ish
        df[col] = (
            s.astype(str).str.upper().map({
                "YES": True, "NO": False,
                "TRUE": True, "FALSE": False,
                "1": True, "2": False, "0": False
            }).astype("boolean")
        )

# ----- High_Volume_and_Low_Complex_or_not → boolean -----
if "High_Volume_and_Low_Complex_or_not" in df.columns:
    df["High_Volume_and_Low_Complex_or_not"] = (
        df["High_Volume_and_Low_Complex_or_not"]
        .astype(str).str.upper()
        .map({"YES": True, "NO": False})
        .astype("boolean")
    )

# ----- day_working → clean B→N then boolean -----
if "day_working" in df.columns:
    df["day_working"] = df["day_working"].replace({"B": "N"})
    df["day_working"] = (
        df["day_working"].astype(str).str.upper()
        .map({"Y": True, "N": False})
        .astype("boolean")
    )

# ----- Pseudo* → category -----
pseudo_cols = [c for c in df.columns if c.startswith("Pseudo")]
if pseudo_cols:
    df[pseudo_cols] = df[pseudo_cols].astype("category")

# === Inspect a few rows of key fields ===
preview_cols = [
    "Year","Operation_Age","IMD_Score","Previous_Operation_Length",
    "Sex","Intended_Management","Heart_Condition","Hypertension",
    "Obesity","Diabetes","Cancer","Chronic_Kidney_Disease",
    "High_Volume_and_Low_Complex_or_not","day_working"
]
preview_cols = [c for c in preview_cols if c in df.columns]

print("\nPost-coercion samples:")
if preview_cols:
    print(df[preview_cols].head(3))
else:
    print("(No preview columns found)")

# === Column dtypes AFTER & change report ===
print("\n=== Column dtypes AFTER ===")
after_dtypes = df.dtypes.astype(str)
print(after_dtypes.sort_index())
print("\nAFTER summary:", after_dtypes.value_counts().to_dict())

# Show only columns whose dtype changed
changes = (
    pd.DataFrame({"before": before_dtypes, "after": after_dtypes})
      .query("before != after")
      .sort_index()
)
print("\n=== DTYPE CHANGES (before → after) ===")
if not changes.empty:
    print(changes)
else:
    print("No dtype changes detected.")

# Handy lists for downstream code/logging
from pandas.api.types import is_bool_dtype, is_numeric_dtype, is_categorical_dtype

# Correct type lists
final_bool_cols = [c for c in df.columns if is_bool_dtype(df[c])]
final_numeric_cols = [c for c in df.columns if is_numeric_dtype(df[c]) and not is_bool_dtype(df[c])]
final_category_cols = [c for c in df.columns if is_categorical_dtype(df[c])]

print("\nBool cols:", final_bool_cols)
print("Numeric cols:", final_numeric_cols)
print("Category cols (incl. Pseudo*):", final_category_cols)

"""### Remove cols with > 50% missingess"""

### Remove cols with > 50% missingess

# print them first
print(df.columns[df.isna().mean() > 0.5])

if Remove_missing_50pc:
    missing_cols = df.columns[df.isna().mean() > 0.5]
    df.drop(columns=missing_cols, inplace=True)

"""### Convert operation lengths to numeric and remove > 600 mins"""

import pandas as pd

def check_and_convert_to_numeric(df, colname, large_threshold=600, as_nan=False):
    print(f"\n=== Checking column: {colname} ===")
    print(f"Original dtype: {df[colname].dtype}")

    # Convert if not already numeric
    if not pd.api.types.is_numeric_dtype(df[colname]):
        pre_numeric_series = pd.to_numeric(df[colname], errors='coerce')
        non_numeric_mask = df[colname].notna() & pre_numeric_series.isna()
        non_numeric_count = non_numeric_mask.sum()
        if non_numeric_count > 0:
            print(f"Found {non_numeric_count} non-numeric values — setting those to NaN.")
            print("Sample non-numeric values:", df.loc[non_numeric_mask, colname].head(5).to_list())
        df[colname] = pre_numeric_series
    else:
        print(f"'{colname}' is already numeric — no type conversion needed.")

    before = len(df)

    if as_nan:
        # Convert > threshold to NaN
        mask = df[colname] > large_threshold
        df.loc[mask, colname] = np.nan
        print(f"Set {mask.sum()} values > {large_threshold} mins to NaN (rows retained).")
    else:
        # Drop rows entirely
        drop_mask = df[colname] > large_threshold
        dropped = int(drop_mask.sum())
        df.drop(df[drop_mask].index, inplace=True)
        print(f"Dropped {dropped} rows with {colname} > {large_threshold} mins.")

    after = len(df)

    # Stats
    print(f"'{colname}' dtype after conversion: {df[colname].dtype}")
    print(f"Rows before: {before:,}, after: {after:,}")
    print(f"Remaining non-null {colname}: {df[colname].notna().sum():,}")

# Example usage
check_and_convert_to_numeric(df, 'Actual_Length', as_nan=False)         # drop rows > 600
check_and_convert_to_numeric(df, 'Previous_Operation_Length', as_nan=True)  # set >600 to NaN

"""### Convert -ve Lengths to +ve"""

df['Actual_Length'] = pd.to_numeric(df['Actual_Length'], errors='coerce')
df['Previous_Operation_Length'] = pd.to_numeric(df['Previous_Operation_Length'], errors='coerce')

# Count how many were converted to numeric (non-NaN after coercion)
actual_numeric_count = df['Actual_Length'].notna().sum()
prev_numeric_count   = df['Previous_Operation_Length'].notna().sum()

# Count how many were negative before conversion
neg_actual_count = (df['Actual_Length'] < 0).sum()
neg_prev_count   = (df['Previous_Operation_Length'] < 0).sum()

# Apply absolute value
df['Actual_Length'] = df['Actual_Length'].abs()
df['Previous_Operation_Length'] = df['Previous_Operation_Length'].abs()

# Print summary
print(f"Converted to numeric: Actual_Length={actual_numeric_count}, Previous_Operation_Length={prev_numeric_count}")
print(f"\nNegative values corrected: Actual_Length={neg_actual_count}, Previous_Operation_Length={neg_prev_count}")

"""### Remove actual length == 0




"""

# Print the count of records before cleaning
before_count = df.shape[0]
actual_length_0_before = df[df['Actual_Length'] == 0].shape[0]

print("Before cleaning:")
print("Total records:", before_count)
print("Records with Actual_Length == 0:", actual_length_0_before)

# Drop only rows with Actual_Length == 0
df = df[df['Actual_Length'] != 0]

# Report remaining count
after_cleaning_count = df.shape[0]
print("Remaining records after dropping Actual_Length == 0:", after_cleaning_count)

"""### Remove Actual Length longer than 600 min"""

# Remove Actual Length longer than 600 min
df = df[df['Actual_Length'] <= 600]

"""### check removal"""

print("Total rows raw:", len(df))
print("Actual_Length == 0:", (df['Actual_Length'] == 0).sum())
print("Actual_Length > 600:", (df['Actual_Length'] > 600).sum())
print("Intended_Management == 0:", (df['Intended_Management'] == 0).sum())
print("Intended_Management == 1:", (df['Intended_Management'] == 1).sum())
print("Intended_Management == 2:", (df['Intended_Management'] == 2).sum())

"""
### Clean Previous Procedure Codes"""

# === EXTENDED CTV3 CLEAN (with per-step prints) + REMAP via TRUD =================
import os, re, zipfile
import pandas as pd, numpy as np

# ── CONFIG ──────────────────────────────────────────────────────────────────────
COLS           = [c for c in ["Previous_Proc_1","Previous_Proc_2","Previous_Proc_3","Previous_Proc_4","Previous_Proc_5"] if c in df.columns]
ZIP_PATH       = "/content/drive/MyDrive/UCL_Dissertation/uk_readctv3xmap_25.0.0_20180401000001.zip"
SAVE_UNMATCHED = True
UNMATCHED_DIR  = "/content/drive/MyDrive/UCL_Dissertation"

print("\n=== EXTENDED CTV3 CLEAN → REMAP (per‑step reporting) ===")
print("Columns:", COLS if COLS else "—")

if not COLS:
    raise SystemExit("No Previous_Proc_* columns present — nothing to do.")

# ── Patterns ───────────────────────────────────────────────────────────────────
# OPCS‑4 standard (A12 or A12.3), and raw (A843 -> A84.3)
OPCS4_FMT_RE  = re.compile(r'^[A-Z][0-9]{2}(?:\.[0-9])?$')
OPCS4_RAW_RE  = re.compile(r'^[A-Z][0-9]{3}$')

# Excel‑style scientific notation corruption like "7.00E+81"
EXCEL_SCI_RE  = re.compile(r'^[0-9A-Z]\.00E\+\d{2}$')

# CTV3: allow 3–6 length pre‑fix (we’ll pad/trim to 5)
CTV3_CORE_RE  = re.compile(r'^[0-9A-Z\.]{3,6}$')

# ── Helpers ────────────────────────────────────────────────────────────────────
def normalise_cell(val):
    if pd.isna(val): return np.nan
    s = str(val).strip()
    if s == "" or s.lower() in {"nan","null","none"}: return np.nan
    return s.upper()

def strip_read_qualifiers_basic(code):
    """Drop common trailing numeric qualifiers like ' 00'/' 1100' and '/1100'."""
    if pd.isna(code): return code
    s = str(code).upper().strip()
    s = re.sub(r'\s+', ' ', s)
    s = re.sub(r'\s+\d{2,4}$', '', s)   # ' 00', ' 11', ' 1100'
    s = re.sub(r'/\d{2,4}$', '', s)     # '/1100'
    return s

def strip_alpha_qualifiers_tail(code):
    """NEW: also drop trailing space + 1–2 alphanumerics (e.g., ' 1E', ' 1V')."""
    if pd.isna(code): return code
    s = str(code).upper().strip()
    s = re.sub(r'\s+[0-9A-Z]{1,2}$', '', s)
    return s

def ctv3_pad_to_5(code):
    """Pad plausible CTV3 cores with dots to length 5 (e.g., '7415' -> '7415.')."""
    if pd.isna(code): return code
    s = str(code).upper().strip()
    if not CTV3_CORE_RE.fullmatch(s):  # if it looks weird, pass through
        return s
    # collapse repeated spaces/dots before sizing
    s = re.sub(r'\s+', '', s)
    s = re.sub(r'\.{3,}', '..', s)
    if len(s) < 5: s = s + "." * (5 - len(s))
    if len(s) > 5: s = s[:5]
    return s

def insert_dot_before_last_digit(opcs_raw):
    """A843 → A84.3; leave A25/A25.1 as‑is; pass non‑OPCS through."""
    if pd.isna(opcs_raw): return np.nan
    s = str(opcs_raw).upper().strip()
    if OPCS4_RAW_RE.fullmatch(s): return f"{s[0]}{s[1:3]}.{s[3]}"
    if OPCS4_FMT_RE.fullmatch(s): return s
    return s

def excel_corruption_flag(code):
    if pd.isna(code): return False
    return bool(EXCEL_SCI_RE.fullmatch(str(code)))

def step_change_count(prev, curr):
    return int((prev.fillna("§").astype(str) != curr.fillna("§").astype(str)).sum())

# ── CLEAN (per‑step) ───────────────────────────────────────────────────────────
excel_flags_total = 0
for COL in COLS:
    print(f"\n[CLEAN] {COL}:")
    s0 = df[COL].copy()
    nonnull = int(s0.notna().sum())
    print(f" - Non‑null: {nonnull:,}")

    # Step 1: normalise
    s1 = s0.map(normalise_cell)
    ch1 = step_change_count(s0, s1)
    print(f"Step 1 (normalise):        {ch1:,} changed ({(ch1 / max(nonnull,1)):.2%} of non‑null)")

    # Step 2: strip numeric qualifiers
    s2 = s1.map(strip_read_qualifiers_basic)
    ch2 = step_change_count(s1, s2)
    print(f"Step 2 (strip qualifiers): {ch2:,} changed")

    # Step 3: strip alpha tail (e.g., ' 1E', ' 1V')
    s3 = s2.map(strip_alpha_qualifiers_tail)
    ch3 = step_change_count(s2, s3)
    print(f"Step 3 (strip alpha tail): {ch3:,} changed")

    # Step 4: CTV3 pad/trim to 5 chars
    s4 = s3.map(ctv3_pad_to_5)
    ch4 = step_change_count(s3, s4)
    print(f"Step 4 (CTV3 pad→5):       {ch4:,} changed")

    # Step 5 (optional): ensure any raw 4‑char OPCS get dotted
    s5 = s4.map(insert_dot_before_last_digit)
    ch5 = step_change_count(s4, s5)
    print(f"Step 5 (OPCS raw→dotted):  {ch5:,} changed")

    # Excel corruption tally (post‑clean)
    excel_cnt = int(((s5.notna()) & s5.map(excel_corruption_flag)).sum())
    excel_flags_total += excel_cnt
    print(f"Excel‑like corruptions flagged: {excel_cnt:,}")

    # Show sample across steps (up to 6)
    demo_mask = (s0.fillna("§").astype(str) != s5.fillna("§").astype(str))
    if demo_mask.any():
        show = demo_mask[demo_mask].index[:6]
        demo = pd.DataFrame({"ORIG": s0.loc[show], "STEP1": s1.loc[show], "STEP2": s2.loc[show],
                             "STEP3": s3.loc[show], "STEP4": s4.loc[show], "STEP5": s5.loc[show]})
        print("   Sample (up to 6 rows):")
        print(demo)

    # Overwrite cleaned series back
    df[COL] = s5

print(f"\n✓ CLEAN complete. Excel‑style corruptions (overall): {excel_flags_total:,}")

# ── REMAP via TRUD ─────────────────────────────────────────────────────────────
print("\n=== REMAP (TRUD) after extended clean ===")
with zipfile.ZipFile(ZIP_PATH, 'r') as z:
    if 'V3/Opcs4.v3' not in z.namelist():
        raise FileNotFoundError("Could not find 'V3/Opcs4.v3' in the TRUD zip.")
    raw_lines = z.read('V3/Opcs4.v3').decode('latin1').splitlines()
parts = [ln.split('|') for ln in raw_lines]
map_df = pd.DataFrame(parts, columns=['src_code','opcs4_raw','map_type','col4','col5','col6','col7'])

# prioritise defaults; tiebreaks stable
type_priority = {'D': 0, 'G': 1, 'A': 2, 'R': 3, 'E': 4}
col4_priority = {'C': 0, 'M': 1}

map_df['opcs4_fmt'] = map_df['opcs4_raw'].map(insert_dot_before_last_digit)
map_df = map_df.reset_index().rename(columns={'index':'_idx'})
map_df['_pri'] = map_df.apply(
    lambda r: (type_priority.get(r['map_type'], 9),
               col4_priority.get(r['col4'], 9),
               int(r['_idx'])),
    axis=1
)
best_rows = map_df.sort_values('_pri').groupby('src_code', as_index=False).first()
best_map  = dict(zip(best_rows['src_code'], best_rows['opcs4_fmt']))

def parent_fallback(code):
    if pd.isna(code): return np.nan
    s = str(code)
    # CTV3 parent convention: last char → '.'
    if len(s) == 5 and '.' not in s:
        return best_map.get(s[:-1] + '.')
    return np.nan

overall_need, overall_mapped_now, overall_nonnull, overall_unmatched = 0, 0, 0, 0

for COL in COLS:
    s = df[COL]
    already = s.notna() & s.astype(str).str.fullmatch(OPCS4_FMT_RE)
    need    = s.notna() & ~already

    n_nonnull = int(s.notna().sum())
    n_need    = int(need.sum())
    overall_nonnull += n_nonnull
    overall_need    += n_need

    print(f"\n[MAP] {COL}:")
    print(f" - Non‑null:       {n_nonnull:,}")
    print(f" - Need mapping:   {n_need:,}")

    mapped_exact  = s.map(best_map)
    mapped_parent = s.where(mapped_exact.notna(), s.map(parent_fallback))

    # build final
    final = pd.Series(index=df.index, dtype=object)
    final[already] = s[already]
    final[mapped_exact.notna()] = mapped_exact[mapped_exact.notna()]
    final[mapped_exact.isna() & mapped_parent.notna()] = mapped_parent[mapped_exact.isna() & mapped_parent.notna()]

    mapped_now = int((mapped_exact.notna() | mapped_parent.notna()).sum())
    unmatched_mask = need & ~(mapped_exact.notna() | mapped_parent.notna())
    unmatched_cnt  = int(unmatched_mask.sum())

    print(f" - Mapped now:     {mapped_now:,} ({(mapped_now / max(n_need,1)):.2%})")
    print(f" - Unmatched now:  {unmatched_cnt:,} ({(unmatched_cnt / max(n_need,1)):.2%})")

    # sample mappings
    sample_idx = ((mapped_exact.notna() | mapped_parent.notna()) & ~already)
    if sample_idx.any():
        show = sample_idx[sample_idx].index[:12]
        preview = pd.DataFrame({"ORIG": s.loc[show], "MAPPED": final.loc[show]})
        print("   Sample mappings (up to 12):")
        print(preview)

    # save unmatched list
    if SAVE_UNMATCHED and unmatched_cnt:
        out_path = os.path.join(UNMATCHED_DIR, f"unmatched_{COL}_after_TRUD_map_EXTENDED.csv")
        s[unmatched_mask].value_counts().to_csv(out_path, header=['count'])
        print(f"   Saved unmatched → {out_path}")

    # overwrite
    df[COL] = final
    overall_mapped_now += int(final.notna().sum())
    overall_unmatched  += unmatched_cnt

print("\n=== OVERALL ===")
print(f"Non‑null total:           {overall_nonnull:,}")
print(f"Need mapping (total):     {overall_need:,}")
print(f"Mapped (final non‑null):  {overall_mapped_now:,}")
print(f"Unmatched after extended: {overall_unmatched:,}")
if excel_flags_total:
    print("\n⚠️  Excel‑like corruptions detected (e.g., '7.00E+81'). These cannot be safely auto‑recovered.")
    print("   Re‑extract those fields as text or hand‑map the frequent ones.")

"""### Proc and age imputation"""

import pandas as pd

# 1) Operation_Age → median
if df['Operation_Age'].isna().any():
    med_age = df['Operation_Age'].median()
    df['Operation_Age'] = df['Operation_Age'].fillna(med_age)
    print(f"[Median] Operation_Age: filled NaN with {med_age}")

# 2) Proc_Code_1_Read → surgeon’s mode
# We’ll use Pseudo_Surgeon as the surgeon identifier
if df['Proc_Code_1_Read'].isna().any():
    # compute mode per surgeon (excluding NaN)
    mode_per_surgeon = (
        df.dropna(subset=['Proc_Code_1_Read'])
          .groupby('Pseudo_Surgeon')['Proc_Code_1_Read']
          .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)
    )

    # map missing values to that surgeon's mode
    missing_mask = df['Proc_Code_1_Read'].isna()
    df.loc[missing_mask, 'Proc_Code_1_Read'] = (
        df.loc[missing_mask, 'Pseudo_Surgeon'].map(mode_per_surgeon)
    )

    # still missing? fill with global mode
    if df['Proc_Code_1_Read'].isna().any():
        global_mode = df['Proc_Code_1_Read'].mode(dropna=True).iloc[0]
        df['Proc_Code_1_Read'] = df['Proc_Code_1_Read'].fillna(global_mode)
        print(f"[Mode] Proc_Code_1_Read: fallback global mode = {global_mode}")
    else:
        print("[Mode] Proc_Code_1_Read: imputed from surgeon’s most common proc")

# Final check
print("\nRemaining missing values:")
print(df.isna().sum()[df.isna().sum() > 0])

"""## Save df for autogluon-clean and SHAP"""

# ── Save cleaned df for AutoGluon ───────────────────
print("\n=== Saving cleaned dataframe for AutoGluon ===")
clean_shape = df.shape
clean_cols = df.columns.tolist()

print(f"▶ Clean df shape: {clean_shape}")
print(f"▶ Clean df columns ({len(clean_cols)}): {clean_cols}")

# Compare shapes
print("\n=== Shape difference ===")
print(f"Rows: {raw_df_shape[0]} → {clean_shape[0]} "
      f"({raw_df_shape[0] - clean_shape[0]} dropped)")
print(f"Cols: {raw_df_shape[1]} → {clean_shape[1]} "
      f"({clean_shape[1] - raw_df_shape[1]} net change)")

# Compare columns
print("\n=== Column difference ===")
raw_set, clean_set = set(raw_df_cols), set(clean_cols)
removed_cols = sorted(raw_set - clean_set)
added_cols   = sorted(clean_set - raw_set)

print(f"Removed columns ({len(removed_cols)}): {removed_cols}")
print(f"Added columns   ({len(added_cols)}): {added_cols}")

# Save to Drive
df.to_csv('/content/drive/MyDrive/UCL_Dissertation/df_cleaned.csv', index=False)
print("\n✓ Cleaned dataframe saved → df_cleaned.csv")

import os
print(os.listdir("/content/drive/MyDrive/UCL_Dissertation"))

import matplotlib.pyplot as plt

# classify underrun/overrun
df['Underrun'] = df['Actual_Length'] < df['Expected_Length']
df['Overrun']  = df['Actual_Length'] > df['Expected_Length']

# aggregate counts per year
counts = df.groupby('Year').agg(
    underruns=('Underrun', 'sum'),
    overruns=('Overrun', 'sum')
).reset_index()

# plot
x = counts['Year']
width = 0.35  # bar width

plt.figure(figsize=(10,6))
plt.bar(x - width/2, counts['underruns'], width,
        label="Underruns", color='tab:blue', alpha=0.6)
plt.bar(x + width/2, counts['overruns'],  width,
        label="Overruns",  color='tab:orange', alpha=0.6)

plt.xlabel("Year")
plt.ylabel("Count of cases")
plt.title("Underruns vs Overruns per Year")
plt.legend()
plt.grid(True, axis='y', linestyle="--", alpha=0.5)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# find top 2 most common procedure codes
top2_codes = df['Proc_Code_1_Read'].value_counts().nlargest(2).index.tolist()

# loop through and plot
fig, axes = plt.subplots(1, 2, figsize=(14,6), sharey=True)

for ax, code in zip(axes, top2_codes):
    subset = df[df['Proc_Code_1_Read'] == code]
    actual   = subset['Actual_Length'].dropna()
    expected = subset['Expected_Length'].dropna()

    bins = np.arange(0, 301, 5)  # 0–300 min in 5-min bins

    ax.hist(actual, bins=bins, alpha=0.6, label="Actual", color='tab:blue')
    ax.hist(expected, bins=bins, alpha=0.6, label="Expected", color='tab:orange')
    ax.set_title(f"Proc Code {code} (n={len(subset)})")
    ax.set_xlabel("Duration (minutes)")
    ax.set_xlim(0, 300)
    ax.grid(True, linestyle="--", alpha=0.5)

axes[0].set_ylabel("Count (frequency)")
axes[1].legend()
plt.suptitle("Overlapping Histograms of Actual vs Expected Lengths for Top 2 Procedure Codes", y=1.02)
plt.tight_layout()
plt.show()

"""## Imputation for model comparison"""

# === Median imputations (numeric) ===
for col in ['Previous_Operation_Length', 'IMD_Score']:
    if df[col].isna().any():
        med_val = df[col].median()
        df[col] = df[col].fillna(med_val)
        print(f"[Median] {col}: filled NaN with {med_val}")

# === Mode imputations (categorical/boolean-like) ===
for col in [
    'Previous_Proc_1',
    'Pseudo_Anaesthetist',
    'weekday_name',
    'day_working',
    'Season',
    'Anaesthetic_Type_Code',
    'Ethnic_Category'
]:
    if df[col].isna().any():
        mode_val = df[col].mode(dropna=True)[0]
        df[col] = df[col].fillna(mode_val)
        print(f"[Mode] {col}: filled NaN with {mode_val}")

# === Double-check ===
print("\nRemaining missing values (if any):")
print(df.isna().sum()[df.isna().sum() > 0])

"""## TE and OHE"""

!pip install category_encoders --quiet

# === Define feature groups ===
bool_cols = [
    "Chronic_Kidney_Disease", "Sex", "Diabetes", "Obesity", "Hypertension",
    "Heart_Condition", "Intended_Management", "High_Volume_and_Low_Complex_or_not",
    "Cancer", "day_working"
]

numeric_cols = [
    "Operation_Age", "Previous_Operation_Length", "IMD_Score", "Year"
    # "Expected_Length" excluded to avoid leakage
]

low_card_cols = [
    "High_Volume_and_Low_Complexity_Category",
    "Anaesthetic_Type_Code",
    "Ethnic_Category",
    "Season",
    "weekday_name",
]

high_card_cols = [
    "Proc_Code_1_Read",
    "Previous_Proc_1",
    "Theatre_Code",
    "Pseudo_Surgeon",
    "Pseudo_Anaesthetist",
    "Pseudo_Consultant",
]

from sklearn.model_selection import train_test_split, KFold
from category_encoders import TargetEncoder
from sklearn.preprocessing import OneHotEncoder
import numpy as np, pandas as pd
import joblib, os

# ==== CONFIG ====
target_col     = "Actual_Length"
random_state   = 42
n_splits       = 5
save_dir       = "/content/drive/MyDrive/UCL_Dissertation"
os.makedirs(save_dir, exist_ok=True)

print("\n=== STARTING PREPROCESSING PIPELINE ===")

# ==== SPLIT ====
print("\n[Split] Creating train/val/test splits...")
X = df.drop(columns=[target_col])
y = df[target_col]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=random_state, shuffle=True
)
X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=random_state, shuffle=True
)
print(f"Train rows: {X_train.shape[0]}, Val rows: {X_val.shape[0]}, Test rows: {X_test.shape[0]}")

# Inspect columns after split
col_summary = inspect_columns(X_train, n_samples=5)
pd.set_option("display.max_colwidth", 120)
print(col_summary.to_string(index=False))

# ==== DEFINE GROUPS ====
def cols_present(df_like, cols):
    missing = [c for c in cols if c not in df_like.columns]
    if missing:
        print(f"⚠️ Skipping missing columns: {missing}")
    return [c for c in cols if c in df_like.columns]

print("\n[Columns] Checking which groups exist in training data...")
bool_cols      = cols_present(X_train, bool_cols)
low_card_cols  = cols_present(X_train, low_card_cols)
high_card_cols = cols_present(X_train, high_card_cols)
numeric_cols   = cols_present(X_train, numeric_cols)
print(f"Bool cols: {bool_cols}")
print(f"Low-card cols: {low_card_cols}")
print(f"High-card cols: {high_card_cols}")
print(f"Numeric cols: {numeric_cols}")

# ==== OOF TARGET ENCODING FOR TRAIN ====
def oof_target_encode(X_tr, y_tr, cols, n_splits=5, random_state=42, smoothing=10.0):
    X_enc = X_tr.copy()
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)

    for col in cols:
        print(f"[TE-OOF] Encoding column: {col}")
        out_col = col + "_te"
        X_enc[out_col] = np.nan
        for fold, (tr_idx, va_idx) in enumerate(kf.split(X_tr), 1):
            print(f"  - Fold {fold}: train={len(tr_idx)}, val={len(va_idx)}")
            te = TargetEncoder(cols=[col], smoothing=smoothing,
                               handle_unknown='value', handle_missing='value')
            te.fit(X_tr.iloc[tr_idx][[col]], y_tr.iloc[tr_idx])
            X_enc.loc[X_tr.index[va_idx], out_col] = te.transform(X_tr.iloc[va_idx][[col]])[col]
    return X_enc

print("\n[TE-OOF] Applying out-of-fold target encoding to TRAIN...")
X_train_te = oof_target_encode(X_train, y_train, high_card_cols,
                               n_splits=n_splits, random_state=random_state)

# ==== FIT TE ON FULL-TRAIN FOR VAL/TEST ====
print("\n[TE-FULL] Fitting target encoders on full TRAIN for use on VAL/TEST...")
te_full = {}
for col in high_card_cols:
    print(f"[TE-FULL] Fitting on column: {col}")
    te = TargetEncoder(cols=[col], smoothing=10.0,
                       handle_unknown='value', handle_missing='value')
    te.fit(X_train[[col]], y_train)
    te_full[col] = te

def apply_te(X_in, te_dict, cols):
    X_out = X_in.copy()
    for col in cols:
        print(f"[TE-APPLY] Applying TE to column: {col}")
        X_out[col + "_te"] = te_dict[col].transform(X_in[[col]])[col]
    return X_out

X_val_te  = apply_te(X_val,  te_full, high_card_cols)
X_test_te = apply_te(X_test, te_full, high_card_cols)

# ==== ONE-HOT ENCODE BOOL + LOW-CARD ====
print("\n[OHE] Fitting OneHotEncoder on TRAIN (bool + low-card cols)...")
ohe_cols = bool_cols + low_card_cols
ohe = OneHotEncoder(handle_unknown="ignore", sparse_output=False, dtype=np.uint8)
ohe.fit(X_train_te[ohe_cols])
print(f"[OHE] Categories per col: {dict(zip(ohe_cols, ohe.categories_))}")

def ohe_df(ohe, X_sub, cols, index):
    arr = ohe.transform(X_sub[cols])
    return pd.DataFrame(arr, index=index, columns=ohe.get_feature_names_out(cols))

print("[OHE] Transforming train/val/test...")
ohe_train = ohe_df(ohe, X_train_te, ohe_cols, X_train_te.index)
ohe_val   = ohe_df(ohe, X_val_te,   ohe_cols, X_val_te.index)
ohe_test  = ohe_df(ohe, X_test_te,  ohe_cols, X_test_te.index)

# ==== BUILD FINAL MATRICES (NUMERIC + OHE + TE) ====
print("\n[Final] Concatenating numeric + OHE + TE...")
te_cols_out = [c + "_te" for c in high_card_cols]
X_train_final = pd.concat([X_train_te[numeric_cols], ohe_train, X_train_te[te_cols_out]], axis=1)
X_val_final   = pd.concat([X_val_te[numeric_cols],   ohe_val,   X_val_te[te_cols_out]],   axis=1)
X_test_final  = pd.concat([X_test_te[numeric_cols],  ohe_test,  X_test_te[te_cols_out]],  axis=1)

# Align val/test to train columns
X_val_final  = X_val_final.reindex(columns=X_train_final.columns, fill_value=0)
X_test_final = X_test_final.reindex(columns=X_train_final.columns, fill_value=0)

print(f"[Final] Shapes: Train={X_train_final.shape}, Val={X_val_final.shape}, Test={X_test_final.shape}")

# ==== SAVE ====
print("\n[Save] Writing outputs to disk...")
train_path = os.path.join(save_dir, "train_preproc.csv")
val_path   = os.path.join(save_dir, "val_preproc.csv")
test_path  = os.path.join(save_dir, "test_preproc.csv")

X_train_final.assign(**{target_col: y_train.values}).to_csv(train_path, index=False)
X_val_final.assign(**{target_col: y_val.values}).to_csv(val_path, index=False)
X_test_final.assign(**{target_col: y_test.values}).to_csv(test_path, index=False)  # include target in test

joblib.dump(ohe, os.path.join(save_dir, "ohe_full_train.joblib"))
joblib.dump(te_full, os.path.join(save_dir, "te_full_train.pkl"))

print("\n[Done] Saved:")
print(" -", train_path)
print(" -", val_path)
print(" -", test_path)
print(" - ohe_full_train.joblib")
print(" - te_full_train.pkl")

"""# graph"""

import matplotlib.pyplot as plt
import matplotlib.lines as mlines

col = "Proc_Code_1_Read"
target = "Actual_Length"
rare_threshold = 20

# Frequency and rare codes
freqs = df[col].value_counts()
rare_codes = set(freqs[freqs < rare_threshold].index)
n_unique = df[col].nunique(dropna=True)

# Sort codes by mean duration
code_means = df.groupby(col)[target].mean().sort_values(ascending=False)
code_to_x = {c: i for i, c in enumerate(code_means.index)}

# Scatter data
x_vals = df[col].map(code_to_x)
y_vals = df[target]
colors = df[col].apply(lambda c: "red" if c in rare_codes else "blue")

plt.figure(figsize=(8,8))

# Plot all cases
plt.scatter(x_vals, y_vals, c=colors, alpha=0.4, s=10)

# Overlay means (common only)
common_codes = [c for c in code_means.index if c not in rare_codes]
plt.scatter(
    [code_to_x[c] for c in common_codes],
    code_means.loc[common_codes].values,
    c="black", s=15, marker="x", zorder=3
)

# Global mean line
gmean = df[target].mean()
plt.axhline(gmean, color="black", linestyle="--", linewidth=1)

# Labels
plt.xlabel(f"Procedure code (index, N={n_unique})")
plt.ylabel("Actual Length (minutes)")
plt.title("Distribution of Actual Length by Procedure Code\n(blue = common, red = rare, black × = common mean)")

# Legend handles
blue_dot = mlines.Line2D([], [], color="blue", marker="o", linestyle="None", markersize=6, label=f"Common procedure (≥{rare_threshold} cases)")
red_dot = mlines.Line2D([], [], color="red", marker="o", linestyle="None", markersize=6, label=f"Rare procedure (<{rare_threshold} cases)")
black_x = mlines.Line2D([], [], color="black", marker="x", linestyle="None", markersize=6, label="Common mean")
mean_line = mlines.Line2D([], [], color="black", linestyle="--", linewidth=1, label="Global mean")

plt.legend(handles=[blue_dot, red_dot, black_x, mean_line])

# Remove x ticks
plt.xticks([], [])

plt.tight_layout()
plt.show()