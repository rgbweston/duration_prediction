# -*- coding: utf-8 -*-
"""autogluon-full_1.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Qz-mPMEDUSaEADPKRecMIAbThyJcptak
"""

!pip install autogluon --quiet

# === AutoGluon best_quality on df_cleaned.csv (no manual split), GPU-if-available, resumable 10‑min chunks (AG 1.4.x) ===
import os, json, time
import pandas as pd
from datetime import datetime, timezone
from autogluon.tabular import TabularPredictor
from google.colab import drive

drive.mount('/content/drive')

BASE_DIR   = "/content/drive/MyDrive/UCL_Dissertation"
CSV_PATH   = f"{BASE_DIR}/df_cleaned.csv"
SAVE_PATH  = f"{BASE_DIR}/ag_clean_bestq_resumable"
TARGET     = "Actual_Length"
TOTAL_CAP  = 5 * 3600         # 5 hours total across sessions
CHUNK_SECS = 10 * 60          # 10-minute chunks
TIMER_JSON = os.path.join(SAVE_PATH, "elapsed_time.json")
LOG_CSV    = os.path.join(SAVE_PATH, "progress_log.csv")

os.makedirs(SAVE_PATH, exist_ok=True)

# ---- GPU detect (PyTorch CUDA presence is a good proxy) ----
try:
    import torch
    NUM_GPUS = torch.cuda.device_count() if torch.cuda.is_available() else 0
except Exception:
    NUM_GPUS = 0
print(f"Detected GPUs: {NUM_GPUS}")

# Load data
assert os.path.exists(CSV_PATH), f"Not found: {CSV_PATH}"
df = pd.read_csv(CSV_PATH)
if "Expected_Length" in df.columns:
    df = df.drop(columns=["Expected_Length"])
print("Loaded df_cleaned:", df.shape)

# Load/init predictor
try:
    predictor = TabularPredictor.load(SAVE_PATH, verbosity=2)
    is_new = False
    print("Loaded existing predictor from SAVE_PATH.")
except Exception:
    predictor = TabularPredictor(
        label=TARGET, path=SAVE_PATH, problem_type="regression", eval_metric="mae"
    )
    is_new = True
    print("Initialized new predictor.")

# elapsed bookkeeping
elapsed = 0.0
if os.path.exists(TIMER_JSON):
    with open(TIMER_JSON, "r") as f:
        elapsed = float(json.load(f).get("elapsed", 0.0))

def save_elapsed(sec: float):
    with open(TIMER_JSON, "w") as f:
        json.dump({"elapsed": sec}, f)

def _metric_info(pred: TabularPredictor):
    """Return (metric_name, greater_is_better: bool)."""
    # predictor.eval_metric can be a string or object depending on version
    metric_obj = getattr(pred, "eval_metric", None)
    name = getattr(metric_obj, "name", None) or str(metric_obj)
    gib  = getattr(metric_obj, "greater_is_better", None)
    if gib is None:
        # Fallback heuristic: common regression metrics
        lower_is_better = {"mae", "mse", "rmse", "rmsle", "mape", "median_absolute_error"}
        gib = not (str(name).lower() in lower_is_better)
    return name, bool(gib)

def _humanize_score(score_val: float, greater_is_better: bool) -> float:
    """Convert AG 'higher-is-better' score to human-friendly: errors positive."""
    return score_val if greater_is_better else -score_val

def log_progress(elapsed_sec: float, chunk_secs: float, fit_time_sec_last: float):
    ts = datetime.now(timezone.utc).isoformat()
    row = {
        "timestamp": ts,
        "elapsed_min": round(elapsed_sec / 60.0, 2),
        "chunk_secs": int(chunk_secs),
        "fit_time_sec_last": round(float(fit_time_sec_last), 2),
        "num_gpus": int(NUM_GPUS),
        "df_rows": int(len(df)),
    }

    # Try to read leaderboard; be robust if empty/errored
    try:
        lb = predictor.leaderboard(silent=True)
        row["num_models"] = int(len(lb))
        metric_name, gib = _metric_info(predictor)
        row["eval_metric"] = metric_name
        row["greater_is_better"] = gib

        if len(lb):
            # best by AG convention (higher is better regardless of metric type)
            best_row = lb.sort_values("score_val", ascending=False).iloc[0]
            row["best_model"] = best_row.get("model", "")
            row["val_score_ag"] = float(best_row.get("score_val", float("nan")))
            row["val_score_human"] = _humanize_score(row["val_score_ag"], gib)
        else:
            row["best_model"] = ""
            row["val_score_ag"] = float("nan")
            row["val_score_human"] = float("nan")
            row["eval_metric"] = row.get("eval_metric", "unknown")
            row["greater_is_better"] = row.get("greater_is_better", True)
    except Exception as e:
        # Minimal log if leaderboard not available
        row.update({
            "num_models": 0,
            "best_model": "",
            "eval_metric": "unknown",
            "greater_is_better": True,
            "val_score_ag": float("nan"),
            "val_score_human": float("nan"),
            "lb_error": str(e),
        })

    # Append to CSV
    header_needed = not os.path.exists(LOG_CSV)
    pd.DataFrame([row]).to_csv(
        LOG_CSV,
        mode="a" if not header_needed else "w",
        header=header_needed,
        index=False,
    )
    print("Logged:", row)

print(f"Starting/resuming. Elapsed so far: {elapsed/3600:.2f} h of {TOTAL_CAP/3600:.2f} h.")

# ---- chunked loop ----
while elapsed < TOTAL_CAP:
    remaining = TOTAL_CAP - elapsed
    this_chunk = min(CHUNK_SECS, remaining)

    t0 = time.time()
    if is_new or not getattr(predictor, "is_fit", False):
        # First fit
        predictor.fit(
            train_data=df,               # AG makes its own holdout
            presets="best_quality",
            time_limit=this_chunk,
            num_gpus=NUM_GPUS,
            num_cpus='auto',
        )
        is_new = False
    else:
        # Subsequent top-ups
        predictor.fit_extra(
            hyperparameters="default",
            ag_args_fit={"max_time_limit": this_chunk},
            num_gpus=NUM_GPUS,
            num_cpus='auto',
            verbosity=2,
        )
    t1 = time.time()

    last_fit_dur = t1 - t0
    elapsed += last_fit_dur
    save_elapsed(elapsed)
    log_progress(elapsed_sec=elapsed, chunk_secs=this_chunk, fit_time_sec_last=last_fit_dur)

    if remaining <= CHUNK_SECS:
        break

print("Done. Total elapsed:", f"{elapsed/3600:.2f} h")
print("Tip: In the fit logs, look for 'gpus=1' in the fold trainers to confirm GPU usage.")

from autogluon.tabular import TabularPredictor
SAVE_PATH = "/content/drive/MyDrive/UCL_Dissertation/ag_clean_bestq_resumable"
predictor = TabularPredictor.load(SAVE_PATH)
lb = predictor.leaderboard(silent=True)
print(lb.head(10))

import pandas as pd

BASE_DIR   = "/content/drive/MyDrive/UCL_Dissertation"
X_test = pd.read_csv(f"{BASE_DIR}/X_test.csv")
y_test = pd.read_csv(f"{BASE_DIR}/y_test.csv").squeeze()  # squeeze → Series not DataFrame

# === AutoGluon feature importance + evaluation (fixed) ===
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error
from scipy.stats import pearsonr
from autogluon.tabular import TabularPredictor

# Paths
BASE_DIR  = "/content/drive/MyDrive/UCL_Dissertation"
SAVE_PATH = f"{BASE_DIR}/ag_clean_bestq_resumable"

# Load predictor + test split
predictor = TabularPredictor.load(SAVE_PATH)
LABEL = predictor.label  # should be "Actual_Length"

X_test = pd.read_csv(f"{BASE_DIR}/X_test.csv")
y_test = pd.read_csv(f"{BASE_DIR}/y_test.csv").squeeze()

# Ensure index alignment and build eval df that includes the label
y_test = y_test.reindex(X_test.index)
df_eval = X_test.copy()
df_eval[LABEL] = y_test.values

# 1) Permutation Feature Importance (requires label present in data)
fi_df = predictor.feature_importance(
    data=df_eval,
    num_shuffle_sets=2,        # was 3 → 1 (3x faster)
    subsample_size=2000        # was 10000 → 2000 rows (5x faster)
)


# Save ranked feature importance table
fi_ranked = fi_df.sort_values("importance", ascending=False).reset_index().rename(columns={"index":"feature"})
print("All features by importance:\n", fi_ranked)
fi_ranked.to_csv(f"{SAVE_PATH}/feature_importance.csv", index=False)

# 2) Predictions
y_pred = predictor.predict(X_test)

# 3) Evaluation block
def eval_block(y_true, y_pred, label="TEST"):
    mae   = mean_absolute_error(y_true, y_pred)
    rmse  = mean_squared_error(y_true, y_pred, squared=False)
    r2    = r2_score(y_true, y_pred)
    medae = median_absolute_error(y_true, y_pred)
    pr, _ = pearsonr(y_true, y_pred) if len(y_true) > 1 else (np.nan, np.nan)

    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)

    def win_acc(pct):
        tol_actual = (pct/100.0) * y_true
        within_actual = np.mean(np.abs(y_pred - y_true) <= tol_actual)
        tol_pred = (pct/100.0) * y_pred
        within_pred = np.mean(np.abs(y_pred - y_true) <= np.abs(tol_pred))
        return within_actual, within_pred

    w5a,  w5p  = win_acc(5)
    w10a, w10p = win_acc(10)

    print(f"[{label}] MAE={mae:.3f} | RMSE={rmse:.3f} | R2={r2:.3f} | MedAE={medae:.3f} | Pearson r={pr:.4f}")
    print(f"       Acc±5% (actual)={w5a*100:.2f}% | Acc±5% (pred)={w5p*100:.2f}%")
    print(f"       Acc±10% (actual)={w10a*100:.2f}% | Acc±10% (pred)={w10p*100:.2f}%")

    return {
        "MAE": mae, "RMSE": rmse, "R2": r2, "MedianAE": medae, "Pearson_r": pr,
        "Acc±5%(actual)": w5a, "Acc±5%(pred)": w5p,
        "Acc±10%(actual)": w10a, "Acc±10%(pred)": w10p
    }

metrics = eval_block(y_test, y_pred, label="TEST")
pd.DataFrame([metrics]).to_csv(f"{SAVE_PATH}/test_metrics.csv", index=False)
print(f"\n✓ Saved FI → {SAVE_PATH}/feature_importance.csv")
print(f"✓ Saved metrics → {SAVE_PATH}/test_metrics.csv")

# === Regression + tolerance metrics only (compatibility fixed) ===
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, median_absolute_error

def eval_block_simple(y_true, y_pred, label="TEST"):
    mae   = mean_absolute_error(y_true, y_pred)
    mse   = mean_squared_error(y_true, y_pred)
    rmse  = np.sqrt(mse)   # instead of squared=False
    r2    = r2_score(y_true, y_pred)
    medae = median_absolute_error(y_true, y_pred)

    y_true = np.asarray(y_true)
    y_pred = np.asarray(y_pred)

    def win_acc(pct):
        tol_actual = (pct/100.0) * y_true
        within_actual = np.mean(np.abs(y_pred - y_true) <= tol_actual)
        tol_pred = (pct/100.0) * y_pred
        within_pred = np.mean(np.abs(y_pred - y_true) <= np.abs(tol_pred))
        return within_actual, within_pred

    w5a,  w5p  = win_acc(5)
    w10a, w10p = win_acc(10)

    print(f"[{label}] MAE={mae:.3f} | RMSE={rmse:.3f} | R2={r2:.3f} | MedianAE={medae:.3f}")
    print(f"       Acc±5% (actual)={w5a*100:.2f}% | Acc±5% (pred)={w5p*100:.2f}%")
    print(f"       Acc±10% (actual)={w10a*100:.2f}% | Acc±10% (pred)={w10p*100:.2f}%")

    return {
        "MAE": mae, "RMSE": rmse, "R2": r2, "MedianAE": medae,
        "Acc±5%(actual)": w5a, "Acc±5%(pred)": w5p,
        "Acc±10%(actual)": w10a, "Acc±10%(pred)": w10p
    }

# Run on test set
metrics = eval_block_simple(y_test, y_pred, label="TEST")

# Save results
pd.DataFrame([metrics]).to_csv(f"{SAVE_PATH}/test_metrics.csv", index=False)
print(f"✓ Saved metrics → {SAVE_PATH}/test_metrics.csv")

# === Plot validation score/error over time (robust to old/new logs) ===
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

LOG_CSV = "/content/drive/MyDrive/UCL_Dissertation/ag_clean_bestq_resumable/progress_log.csv"
assert os.path.exists(LOG_CSV), f"Not found: {LOG_CSV}"

df = pd.read_csv(LOG_CSV)
print("Log columns:", df.columns.tolist())
print("Log head:\n", df.head())

# Coerce types
if "elapsed_min" in df.columns:
    df["elapsed_min"] = pd.to_numeric(df["elapsed_min"], errors="coerce")
else:
    # fallback if only timestamps exist
    df["elapsed_min"] = (pd.to_datetime(df["timestamp"], errors="coerce") - pd.to_datetime(df["timestamp"].iloc[0])).dt.total_seconds()/60

# Prefer latest entry when duplicate elapsed_min occurs
df = df.sort_values(["elapsed_min", "timestamp" if "timestamp" in df.columns else df.index.name]).drop_duplicates("elapsed_min", keep="last")

# Determine metric name and direction if available
metric_name = (df["eval_metric"].iloc[-1] if "eval_metric" in df.columns and pd.notna(df["eval_metric"].iloc[-1]) else "validation")
gib_series = df["greater_is_better"] if "greater_is_better" in df.columns else None
gib = (bool(gib_series.iloc[-1]) if gib_series is not None and pd.notna(gib_series.iloc[-1]) else None)

# Build a human-friendly series:
# Priority: 'val_score_human' (already positive for errors) → compute from 'val_score_ag' + direction →
# fallback to legacy 'val_score(higher_is_better)' (guess direction)
human_col = None
if "val_score_human" in df.columns:
    human = pd.to_numeric(df["val_score_human"], errors="coerce")
    human_col = "val_score_human"
elif "val_score_ag" in df.columns:
    ag = pd.to_numeric(df["val_score_ag"], errors="coerce")
    if gib is None:
        # Heuristic: if metric looks like an error (mae/mse/rmse/med), flip sign
        err_like = any(x in str(metric_name).lower() for x in ["mae", "mse", "rmse", "rmsle", "mape", "median"])
        human = (-ag if err_like else ag)
    else:
        human = (ag if gib else -ag)
    human_col = "val_score_ag→human"
elif "val_score(higher_is_better)" in df.columns:
    legacy = pd.to_numeric(df["val_score(higher_is_better)"], errors="coerce")
    # If values are all negative, assume it's an error metric stored in AG convention → flip to positive
    if legacy.max() <= 0:
        human = -legacy
    else:
        human = legacy
    human_col = "legacy→human"
else:
    raise RuntimeError("No score column found: expected one of ['val_score_human','val_score_ag','val_score(higher_is_better)'].")

# Label for y-axis
mn = str(metric_name).upper()
if gib is None:
    # Best-effort label
    ylab = f"{mn} (human-friendly)"
else:
    ylab = f"{mn} ({'higher is better' if gib else 'lower is better'})"

plt.figure(figsize=(8,5))
plt.plot(df["elapsed_min"], human, marker="o")
plt.xlabel("Elapsed time (minutes)")
plt.ylabel(ylab)
plt.title("AutoGluon validation over time")
plt.grid(True)
plt.tight_layout()
plt.show()

