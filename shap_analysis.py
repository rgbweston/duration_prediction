# -*- coding: utf-8 -*-
"""SHAP_1.0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xIY975vSTkirBWqgmdo690pqymQlHnkH

# Define your path and set training rounds (for debug and marking)
"""

N_TRIALS = 1    # use 1 for quick debug, 1000 for full training
path_to_clean_df = '__'

"""# Load student file from google drive"""

import numpy as np
import pandas as pd
'''
# ── Mount Google Drive ──────────────────────────────
from google.colab import drive
drive.mount('/content/drive')


import os
'''
# ── Load raw merged dataset ─────────────────────────
path = path_to_clean_df
df = pd.read_csv(path)

"""# Create Overrun Target"""

# ── Create Overrun target ───────────────────────────
df["Overrun"] = (df["Actual_Length"] > df["Expected_Length"]).astype(int)

# Define features (drop leakage col)
drop_cols = ["Expected_Length"]
features = df.drop(columns=["Actual_Length", "Overrun"] + drop_cols).columns.tolist()

# Separate regression and classification targets
X = df[features]
y_reg = df["Actual_Length"]
y_clf = df["Overrun"]

print("Features:", len(features))
print("Regression target:", y_reg.shape)

# Print Overrun distribution with labels
counts = y_clf.value_counts(normalize=True).rename("proportion")
print("\nClassification target: Overrun")
print("0 = No overrun (≤ Expected_Length)")
print("1 = Overrun   (> Expected_Length)")
print(counts)

"""# SHAP"""

!pip install --upgrade xgboost

"""# Create evaluation function"""

RESULTS = []

def evaluate_model(name, model, X_test, y_test, is_classifier=False):
    print(f"running evaluation for {name}")
    y_true = np.asarray(y_test).ravel()

    if is_classifier:
        y_pred = model.predict(X_test).ravel()
        y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

        metrics = {
            "Model": name,
            "Accuracy": accuracy_score(y_true, y_pred),
            "Precision": precision_score(y_true, y_pred, zero_division=0),
            "Recall": recall_score(y_true, y_pred, zero_division=0),
            "F1": f1_score(y_true, y_pred, zero_division=0),
            "AUC-ROC": roc_auc_score(y_true, y_pred_proba) if y_pred_proba is not None else None
        }

        auc_roc = f"{metrics['AUC-ROC']:.3f}" if metrics['AUC-ROC'] is not None else "n/a"
        print(f"{name} test: acc={metrics['Accuracy']:.3f} | prec={metrics['Precision']:.3f} | "
              f"rec={metrics['Recall']:.3f} | f1={metrics['F1']:.3f} | auc={auc_roc}")

    else:
        y_pred = model.predict(X_test).ravel()
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        r2 = r2_score(y_true, y_pred)

        def win_acc(pct):
            tol_actual = (pct/100.0) * np.abs(y_true)
            within_a = np.mean(np.abs(y_pred - y_true) <= tol_actual)
            tol_pred = (pct/100.0) * np.abs(y_pred)
            within_p = np.mean(np.abs(y_pred - y_true) <= tol_pred)
            return within_a * 100, within_p * 100

        w5a, w5p = win_acc(5)
        w10a, w10p = win_acc(10)

        metrics = {
            "Model": name, "MAE": mae, "RMSE": rmse, "R2": r2,
            "Acc±5%(actual)": w5a, "Acc±5%(pred)": w5p,
            "Acc±10%(actual)": w10a, "Acc±10%(pred)": w10p
        }

        print(f"{name} test: mae={mae:.3f} | rmse={rmse:.3f} | r2={r2:.3f}")
        print(f"          ±5% acc: actual={w5a:.2f}% | pred={w5p:.2f}%")
        print(f"          ±10% acc: actual={w10a:.2f}% | pred={w10p:.2f}%")

    RESULTS.append(metrics)
    return metrics

"""## Train model"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    mean_absolute_error, mean_squared_error, r2_score,
    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
)
import xgboost as xgb

# set number of boosting rounds
N_TRIALS = 1   # change to 1000 for full training

# load dataset
path = path_to_clean_df
df = pd.read_csv(path)

# create overrun target
df["Overrun"] = (df["Actual_Length"] > df["Expected_Length"]).astype(int)

# handle categoricals
cat_cols = df.select_dtypes(include="object").columns
if len(cat_cols) == 0:
    print("no object columns found for categorical conversion")
else:
    for col in cat_cols:
        df[col] = df[col].astype("category")
    print("converted to category dtype:", list(cat_cols))

# define features + targets
X = df.drop(columns=["Actual_Length", "Overrun", "Expected_Length"])
y_reg = df["Actual_Length"]
y_clf = df["Overrun"].astype(int)

# train/val/test split
X_train, X_temp, y_reg_train, y_reg_temp, y_clf_train, y_clf_temp = train_test_split(
    X, y_reg, y_clf, test_size=0.3, random_state=42, stratify=y_clf
)
X_val, X_test, y_reg_val, y_reg_test, y_clf_val, y_clf_test = train_test_split(
    X_temp, y_reg_temp, y_clf_temp, test_size=0.5, random_state=42, stratify=y_clf_temp
)
print("train/val/test shapes:", X_train.shape, X_val.shape, X_test.shape)

# tuned params
print("same parameters as best-performing xgboost model loaded")
print(f"training with {N_TRIALS} boosting round(s)")
base_params = dict(
    eta=0.015794797197594638,
    max_depth=8,
    min_child_weight=3,
    subsample=0.9010984903770198,
    colsample_bytree=0.5372753218398854,
    gamma=4.9344346830025865,
    reg_alpha=0.2545150013091294,
    reg_lambda=0.009853225172032555,
    n_estimators=N_TRIALS,
    random_state=42,
    n_jobs=-1,
    early_stopping_rounds=50,
    enable_categorical=True,
    tree_method="hist",
    device="cuda"
)

# regression model
print("training regression model")
xgb_reg = xgb.XGBRegressor(**base_params, objective="reg:squarederror", eval_metric="mae")
xgb_reg.fit(X_train, y_reg_train, eval_set=[(X_val, y_reg_val)], verbose=50)

# classification model
print("training classification model")
xgb_clf = xgb.XGBClassifier(**base_params, objective="binary:logistic", eval_metric="logloss")
xgb_clf.fit(X_train, y_clf_train, eval_set=[(X_val, y_clf_val)], verbose=50)

# predictions
y_pred_minutes = xgb_reg.predict(X_test)
y_overrun_pct = xgb_clf.predict_proba(X_test)[:, 1] * 100
out = pd.DataFrame({
    "Actual_Length": y_reg_test.values,
    "Pred_Duration_min": y_pred_minutes,
    "Overrun_Prob_%": y_overrun_pct
}, index=X_test.index)

print("sample predictions:")
print(out.head(10))

# evaluate
evaluate_model("XGB_Regression", xgb_reg, X_test, y_reg_test)
evaluate_model("XGB_Classification", xgb_clf, X_test, y_clf_test, is_classifier=True)

# results summary in memory only
if RESULTS:
    df_results = pd.DataFrame(RESULTS)
    print("results summary:")
    print(df_results)
else:
    print("no results recorded")

"""## Compute SHAP"""

import shap
import numpy as np
import pandas as pd

print("preparing background data for shap")

# keep feature names
feature_names = X_test.columns.tolist()

# encode categoricals
X_train_enc = X_train.copy()
X_test_enc  = X_test.copy()
for col in X_train_enc.select_dtypes(include="category").columns:
    X_train_enc[col] = X_train_enc[col].cat.codes
    X_test_enc[col]  = X_test_enc[col].cat.codes

# force float64
X_train_enc = X_train_enc.astype("float64")
X_test_enc  = X_test_enc.astype("float64")
print("converted categoricals, shapes:", X_train_enc.shape, X_test_enc.shape)

# regression shap
print("explaining regression model")
explainer_reg = shap.TreeExplainer(xgb_reg)
shap_values_reg = explainer_reg.shap_values(X_test_enc, check_additivity=False)
print("regression shap done:", shap_values_reg.shape)

# classification shap
print("explaining classification model")
explainer_clf = shap.TreeExplainer(xgb_clf)
shap_values_clf = explainer_clf.shap_values(X_test_enc, check_additivity=False)
print("classification shap done:", shap_values_clf.shape)

# top regression features
reg_importance = pd.DataFrame({
    "feature": feature_names,
    "mean_abs_shap": np.abs(shap_values_reg).mean(axis=0)
}).sort_values("mean_abs_shap", ascending=False).head(10)

print("\ntop 10 regression features by mean |shap|:")
print(reg_importance)

# top classification features
clf_importance = pd.DataFrame({
    "feature": feature_names,
    "mean_abs_shap": np.abs(shap_values_clf).mean(axis=0)
}).sort_values("mean_abs_shap", ascending=False).head(10)

print("\ntop 10 classification features by mean |shap|:")
print(clf_importance)

import shap
import matplotlib.pyplot as plt

# beeswarm for regression
print("beeswarm plot for regression model")
shap.summary_plot(
    shap_values_reg, X_test_enc, feature_names=feature_names, max_display=15
)

# beeswarm for classification
print("beeswarm plot for classification model")
shap.summary_plot(
    shap_values_clf, X_test_enc, feature_names=feature_names, max_display=15
)

# waterfall for regression: pick first test case
case_idx = X_test_enc.index[0]
print(f"waterfall plot for regression model, case index={case_idx}")
shap.waterfall_plot(
    shap.Explanation(
        values=shap_values_reg[X_test_enc.index.get_loc(case_idx)],
        base_values=explainer_reg.expected_value,
        data=X_test_enc.loc[case_idx].values,
        feature_names=feature_names
    ),
    max_display=20
)